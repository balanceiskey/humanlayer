
[ENG-1520] Implement file snapshot system to capture full content during Read operations
Status: in dev
Assignee: allison@humanlayer.dev

Description:
### problem to solve

Currently the WUI can't show proper diffs because the Read tool often returns partial file content (with offset/limit). We need to capture full file snapshots during Read operations to enable accurate diff display.

## Key Details

* Create new `file_snapshots` SQLite table to store full file content
* Process snapshots on Read tool results (not calls) to ensure file exists
* If Read returns full content (`totalLines == numLines`), use it directly
* If Read is partial, read full file from filesystem
* Store Read metadata (numLines, startLine, totalLines) with snapshot
* Index by session_id and file_path for efficient lookups

## Implementation Notes

Add snapshot capture logic to `processStreamEvent` in `hld/session/manager.go:641-656`. Create migration for new table with appropriate indexes.

## References

* Source: `thoughts/shared/research/2025-07-03_09-43-22_wui_diff_view_tickets.md` ([View on GitHub](https://github.com/humanlayer/thoughts/blob/main/repos/humanlayer/shared/research/2025-07-03_09-43-22_wui_diff_view_tickets.md))
* Related code: `hld/store/sqlite.go:110-145`, `hld/session/manager.go:641-656`
* Parent ticket: [ENG-1502](https://linear.app/humanlayer/issue/ENG-1502/make-tickets-to-improve-diff-views)

Comments:
[2025-07-07 11:25:50] allison@humanlayer.dev:
## Implementation Plan Created

I've created a detailed implementation plan for this ticket at:
`thoughts/shared/plans/file_snapshot_system.md`

### Plan Summary

The plan implements a file snapshot system to capture full file content during Read tool operations, focusing on:

1. **Database Schema & Migration** - Add `file_snapshots` table (migration 7)
2. **Store Methods** - CRUD operations for snapshots
3. **Snapshot Capture Logic** - Async processing of Read tool results
   - Uses tool result content directly when `totalLines == numLines`
   - Reads full file from filesystem for partial reads
   - Stores relative file paths as-is from tool calls

### Key Design Decisions

- **Simplified schema**: Removed hash, size, and Read metadata fields (available from tool result)
- **MVP approach**: No deduplication, no CD command handling
- **Path strategy**: Store relative paths as-is, defer complex matching to future tickets
- **Async capture**: Non-blocking snapshot creation using goroutines
- **Size limit**: 10MB max, falls back to partial content if exceeded

### What's NOT in scope

- RPC endpoint (moved to ENG-1523, see `thoughts/shared/plans/file_snapshot_rpc.md`)
- Path matching logic for Edit operations
- Handling CD commands or working directory changes
- Snapshot correlation with Edit operations

The implementation is ready for review.

[2025-07-07 14:40:01] Sundeep Malladi:
@allison @dexter - Wonder if we should address [ENG-1472](https://linear.app/humanlayer/issue/ENG-1472/refactor-sqlite-migration-system-for-better-maintainability) before we deliver this?

[2025-07-07 14:43:01] Dexter Horthy:
i think 1472 may be happening incidentally / incrementally / opportunistically as we do this?

[2025-07-07 16:07:49] allison@humanlayer.dev:
don't think it's happening incidentaly/incrementally. Also don't think there is much harm in doing this before that either tho. 

[2025-07-08 07:35:02] Dexter Horthy:
i guess my point is, you're adding sqlite migrations to the system as part of this ticket. Which is part of learning how 1472 should look. and if you find ways to improve it as you go, I think that's fine. I try not to think of refactor-y work like 1472 as its own task necessarily - [https://ronjeffries.com/xprog/articles/refactoring-not-on-the-backlog/](https://ronjeffries.com/xprog/articles/refactoring-not-on-the-backlog/)

[2025-07-08 08:07:38] Dexter Horthy:
fwiw i've seen many production go systems use exactly the pattern you have for migrations

[2025-07-08 08:08:08] Sundeep Malladi:
I'm not opposed to these being split into separate tasks, but 1472 is being pitched as a refactor for maintainability and it's not entirely that. It solves a user question as well regarding "What happens when I receive an app update" and eventually if we want that to be durable, I think that code should shift under a library. 

Again, don't need to do it right this second as it doesn't fit under our theme of getting the MVP done, but eventually I think I'd prefer we didn't own migrations.  

[2025-07-08 08:09:36] Dexter Horthy:
> I think that code should shift under a library.

why?

[2025-07-08 08:19:30] Dexter Horthy:
whats the justification on 10MB file size limit? SQLite limitations considered? e.g.

> The maximum number of bytes in a string or BLOB in SQLite is defined by the preprocessor macro SQLITE_MAX_LENGTH. The default value of this macro is 1 billion (1 thousand million or 1,000,000,000). You can raise or lower this value at compile-time using a command-line option like this:

[https://sqlite.org/limits.html](https://sqlite.org/limits.html)

[2025-07-08 08:20:33] Sundeep Malladi:
For a variety of reasons, but top-of-mind for me:

* We've already had one scenario where that code didn't fire in expected ways and required manual, additional SQL commands (I'll note also, I haven't seen one again since)
* As `applyMigrations` grows, it's going to expand the number of if conditional blocks, with steadily more complexity. Now, we might be able to refactor and reduce that complexity, but we'll essentially be rebuilding migration tooling that already exists. Added, it's much easier to review and reason through individual migration files to understand what changes are being applied and when and that will be better-represented in logging as well. When the point comes to determine a customer issue those will turn into real, valuable Sentry errors for us instead of having to delicately drop error logging in the right spots. 
* We're working with quite soft terrain in terms of what Anthropic may remove/add in their API, we can expect DB changes down the road, possibly a lot.

[2025-07-08 08:23:07] Sundeep Malladi:
Let me counterpoint myself a little bit: 

* The Go file is pretty readable, debuggable, etc (disregarding for a moment that the actual SQL itself isn't syntax highlighted)
* `applyMigrations` itself isn't currently humongous or anything
* It's much easier to just add another SQL statement there for the moment then bring in additional tooling

[2025-07-08 08:23:48] Dexter Horthy:
yeah -  for the initial message, agree with 1 and 3 but not fully on board with 2 - i think if we make this 5% better every time we touch it, we don't risk a ton of complexity. I have no problem rebuilding existing tooling if we do it incrementally over time and maintain control. Most migration tools are built for saas apps not on-prem (and desktop app is basically on-prem)

[2025-07-08 08:28:58] allison@humanlayer.dev:
Was just relatively "reasonable" to set a size max. 10MB pretty arbitrary. Mainly that claude code won't read files over 25k tokens itself so storing snapshots of files of that size won't be that productive towards diff viewing. Like if it's over that size then claude will need to read it in pieces anyway. Also the snapshot system stores at every read, so until we did follow up hashing tickets or something I would imagine like 20x reads times 20MB would be a loooot of disk space.


View in Linear: https://linear.app/humanlayer/issue/ENG-1520/implement-file-snapshot-system-to-capture-full-content-during-read
